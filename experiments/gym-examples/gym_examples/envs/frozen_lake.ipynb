{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from os import path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env, logger, spaces, utils\n",
    "from gym.envs.toy_text.utils import categorical_sample\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# DFS to check that it's a valid path.\n",
    "def is_valid(board: List[List[str]], max_size: int) -> bool:\n",
    "    frontier, discovered = [], set()\n",
    "    frontier.append((0, 0))\n",
    "    while frontier:\n",
    "        r, c = frontier.pop()\n",
    "        if not (r, c) in discovered:\n",
    "            discovered.add((r, c))\n",
    "            directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "            for x, y in directions:\n",
    "                r_new = r + x\n",
    "                c_new = c + y\n",
    "                if r_new < 0 or r_new >= max_size or c_new < 0 or c_new >= max_size:\n",
    "                    continue\n",
    "                if board[r_new][c_new] == \"G\":\n",
    "                    return True\n",
    "                if board[r_new][c_new] != \"H\":\n",
    "                    frontier.append((r_new, c_new))\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_random_map(size: int = 8, p: float = 0.8) -> List[str]:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "\n",
    "    Args:\n",
    "        size: size of each side of the grid\n",
    "        p: probability that a tile is frozen\n",
    "\n",
    "    Returns:\n",
    "        A random valid map\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    board = []  # initialize to make pyright happy\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        board = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        board[0][0] = \"S\"\n",
    "        board[-1][-1] = \"G\"\n",
    "        valid = is_valid(board, size)\n",
    "    return [\"\".join(x) for x in board]\n",
    "\n",
    "\n",
    "class FrozenLakeEnv(Env):\n",
    "    \"\"\"\n",
    "    Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H)\n",
    "    by walking over the Frozen(F) lake.\n",
    "    The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "\n",
    "    ### Action Space\n",
    "    The agent takes a 1-element vector for actions.\n",
    "    The action space is `(dir)`, where `dir` decides direction to move in which can be:\n",
    "\n",
    "    - 0: LEFT\n",
    "    - 1: DOWN\n",
    "    - 2: RIGHT\n",
    "    - 3: UP\n",
    "\n",
    "    ### Observation Space\n",
    "    The observation is a value representing the agent's current position as\n",
    "    current_row * nrows + current_col (where both the row and col start at 0).\n",
    "    For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "    The number of possible observations is dependent on the size of the map.\n",
    "    For example, the 4x4 map has 16 possible observations.\n",
    "\n",
    "    ### Rewards\n",
    "\n",
    "    Reward schedule:\n",
    "    - Reach goal(G): +1\n",
    "    - Reach hole(H): 0\n",
    "    - Reach frozen(F): 0\n",
    "\n",
    "    ### Arguments\n",
    "\n",
    "    ```\n",
    "    gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "    ```\n",
    "\n",
    "    `desc`: Used to specify custom map for frozen lake. For example,\n",
    "\n",
    "        desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"].\n",
    "\n",
    "        A random generated map can be specified by calling the function `generate_random_map`. For example,\n",
    "\n",
    "        ```\n",
    "        from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "        gym.make('FrozenLake-v1', desc=generate_random_map(size=8))\n",
    "        ```\n",
    "\n",
    "    `map_name`: ID to use any of the preloaded maps.\n",
    "\n",
    "        \"4x4\":[\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "            ]\n",
    "\n",
    "        \"8x8\": [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FHHFFFHF\",\n",
    "            \"FHFFHFHF\",\n",
    "            \"FFFHFFFG\",\n",
    "        ]\n",
    "\n",
    "    `is_slippery`: True/False. If True will move in intended direction with\n",
    "    probability of 1/3 else will move in either perpendicular direction with\n",
    "    equal probability of 1/3 in both directions.\n",
    "\n",
    "        For example, if action is left and is_slippery is True, then:\n",
    "        - P(move left)=1/3\n",
    "        - P(move up)=1/3\n",
    "        - P(move down)=1/3\n",
    "\n",
    "    ### Version History\n",
    "    * v1: Bug fixes to rewards\n",
    "    * v0: Initial versions release (1.0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        render_mode: Optional[str] = None,\n",
    "        desc=None,\n",
    "        map_name=\"4x4\",\n",
    "        is_slippery=True,\n",
    "    ):\n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc, dtype=\"c\")\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        self.initial_state_distrib = np.array(desc == b\"S\").astype(\"float64\").ravel()\n",
    "        self.initial_state_distrib /= self.initial_state_distrib.sum()\n",
    "\n",
    "        self.P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row * ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col - 1, 0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif a == UP:\n",
    "                row = max(row - 1, 0)\n",
    "            return (row, col)\n",
    "\n",
    "        def update_probability_matrix(row, col, action):\n",
    "            newrow, newcol = inc(row, col, action)\n",
    "            newstate = to_s(newrow, newcol)\n",
    "            newletter = desc[newrow, newcol]\n",
    "            terminated = bytes(newletter) in b\"GH\"\n",
    "            reward = float(newletter == b\"G\")\n",
    "            return newstate, reward, terminated\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = self.P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b\"GH\":\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
    "                                li.append(\n",
    "                                    (1.0 / 3.0, *update_probability_matrix(row, col, b))\n",
    "                                )\n",
    "                        else:\n",
    "                            li.append((1.0, *update_probability_matrix(row, col, a)))\n",
    "\n",
    "        self.observation_space = spaces.Discrete(nS)\n",
    "        self.action_space = spaces.Discrete(nA)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # pygame utils\n",
    "        self.window_size = (min(64 * ncol, 512), min(64 * nrow, 512))\n",
    "        self.cell_size = (\n",
    "            self.window_size[0] // self.ncol,\n",
    "            self.window_size[1] // self.nrow,\n",
    "        )\n",
    "        self.window_surface = None\n",
    "        self.clock = None\n",
    "        self.hole_img = None\n",
    "        self.cracked_hole_img = None\n",
    "        self.ice_img = None\n",
    "        self.elf_images = None\n",
    "        self.goal_img = None\n",
    "        self.start_img = None\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, t = transitions[i]\n",
    "        self.s = s\n",
    "        self.lastaction = a\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return (int(s), r, t, False, {\"prob\": p})\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
    "        self.lastaction = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return int(self.s), {\"prob\": 1}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "        elif self.render_mode == \"ansi\":\n",
    "            return self._render_text()\n",
    "        else:  # self.render_mode in {\"human\", \"rgb_array\"}:\n",
    "            return self._render_gui(self.render_mode)\n",
    "\n",
    "    def _render_gui(self, mode):\n",
    "        try:\n",
    "            import pygame\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[toy_text]`\"\n",
    "            )\n",
    "\n",
    "        if self.window_surface is None:\n",
    "            pygame.init()\n",
    "\n",
    "            if mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                pygame.display.set_caption(\"Frozen Lake\")\n",
    "                self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "            elif mode == \"rgb_array\":\n",
    "                self.window_surface = pygame.Surface(self.window_size)\n",
    "\n",
    "        assert (\n",
    "            self.window_surface is not None\n",
    "        ), \"Something went wrong with pygame. This should never happen.\"\n",
    "\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.hole_img is None:\n",
    "            # file_name = path.join(path.dirname(__file__), \"img/hole.png\")\n",
    "            file_name = \"img/hole.png\"\n",
    "            self.hole_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.cracked_hole_img is None:\n",
    "            # file_name = path.join(path.dirname(__file__), \"img/cracked_hole.png\")\n",
    "            file_name =  \"img/cracked_hole.png\"\n",
    "            self.cracked_hole_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.ice_img is None:\n",
    "            # file_name = path.join(path.dirname(__file__), \"img/ice.png\")\n",
    "            file_name = \"img/ice.png\"\n",
    "            self.ice_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.goal_img is None:\n",
    "            # file_name = path.join(path.dirname(__file__), \"img/goal.png\")\n",
    "            file_name = \"img/goal.png\"\n",
    "            self.goal_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.start_img is None:\n",
    "            # file_name = path.join(path.dirname(__file__), \"img/stool.png\")\n",
    "            file_name = \"img/stool.png\"\n",
    "            self.start_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.elf_images is None:\n",
    "            elfs = [\n",
    "                # path.join(path.dirname(__file__), \"img/elf_left.png\"),\n",
    "                # path.join(path.dirname(__file__), \"img/elf_down.png\"),\n",
    "                # path.join(path.dirname(__file__), \"img/elf_right.png\"),\n",
    "                # path.join(path.dirname(__file__), \"img/elf_up.png\"),\n",
    "                \"img/elf_left.png\",\n",
    "                \"img/elf_down.png\",\n",
    "                \"img/elf_right.png\",\n",
    "                \"img/elf_up.png\",\n",
    "            ]\n",
    "            self.elf_images = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in elfs\n",
    "            ]\n",
    "\n",
    "        desc = self.desc.tolist()\n",
    "        assert isinstance(desc, list), f\"desc should be a list or an array, got {desc}\"\n",
    "        for y in range(self.nrow):\n",
    "            for x in range(self.ncol):\n",
    "                pos = (x * self.cell_size[0], y * self.cell_size[1])\n",
    "                rect = (*pos, *self.cell_size)\n",
    "\n",
    "                self.window_surface.blit(self.ice_img, pos)\n",
    "                if desc[y][x] == b\"H\":\n",
    "                    self.window_surface.blit(self.hole_img, pos)\n",
    "                elif desc[y][x] == b\"G\":\n",
    "                    self.window_surface.blit(self.goal_img, pos)\n",
    "                elif desc[y][x] == b\"S\":\n",
    "                    self.window_surface.blit(self.start_img, pos)\n",
    "\n",
    "                pygame.draw.rect(self.window_surface, (180, 200, 230), rect, 1)\n",
    "\n",
    "        # paint the elf\n",
    "        bot_row, bot_col = self.s // self.ncol, self.s % self.ncol\n",
    "        cell_rect = (bot_col * self.cell_size[0], bot_row * self.cell_size[1])\n",
    "        last_action = self.lastaction if self.lastaction is not None else 1\n",
    "        elf_img = self.elf_images[last_action]\n",
    "\n",
    "        if desc[bot_row][bot_col] == b\"H\":\n",
    "            self.window_surface.blit(self.cracked_hole_img, cell_rect)\n",
    "        else:\n",
    "            self.window_surface.blit(elf_img, cell_rect)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        elif mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.window_surface)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _center_small_rect(big_rect, small_dims):\n",
    "        offset_w = (big_rect[2] - small_dims[0]) / 2\n",
    "        offset_h = (big_rect[3] - small_dims[1]) / 2\n",
    "        return (\n",
    "            big_rect[0] + offset_w,\n",
    "            big_rect[1] + offset_h,\n",
    "        )\n",
    "\n",
    "    def _render_text(self):\n",
    "        desc = self.desc.tolist()\n",
    "        outfile = StringIO()\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(f\"  ({['Left', 'Down', 'Right', 'Up'][self.lastaction]})\\n\")\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(\"\".join(line) for line in desc) + \"\\n\")\n",
    "\n",
    "        with closing(outfile):\n",
    "            return outfile.getvalue()\n",
    "\n",
    "    def close(self):\n",
    "        if self.window_surface is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "\n",
    "# Elf and stool from https://franuka.itch.io/rpg-snow-tileset\n",
    "# All other assets by Mel Tillery http://www.cyaneus.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefflai/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.34     |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -52.5    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.000593 |\n",
      "|    value_loss         | 2.34e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.15     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 281      |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -531     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.301    |\n",
      "|    value_loss         | 0.0556   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.61     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 421      |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -158     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00014  |\n",
      "|    value_loss         | 1.38e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 8.03      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 563       |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -5.72e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.0533   |\n",
      "|    value_loss         | 0.0018    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.88     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 705      |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -1.72    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 3.74e-06 |\n",
      "|    value_loss         | 6.29e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8        |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 846      |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.17     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.000451 |\n",
      "|    value_loss         | 3.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 7.8      |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 987      |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | -244     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00153  |\n",
      "|    value_loss         | 3.98e-06 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m A2C(\u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m)\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/a2c/a2c.py:194\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[39mself\u001b[39m: SelfA2C,\n\u001b[1;32m    187\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfA2C:\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    195\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    196\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    197\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    198\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    199\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    200\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/SpaDS/env/lib/python3.11/site-packages/shimmy/openai_gym_compatibility.py:123\u001b[0m, in \u001b[0;36mGymV26CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "Cell \u001b[0;32mIn[19], line 252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "Cell \u001b[0;32mIn[19], line 279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "Cell \u001b[0;32mIn[19], line 382\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    380\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m    381\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    383\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    385\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    386\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs)\n",
    "while True:\n",
    "# for _ in range(1000):\n",
    "    action, _ = model.predict(obs[0])\n",
    "    print(int(action))\n",
    "    observation, reward, terminated, truncated, info = env.step(int(action))\n",
    "    #reshpare a list to np.array (10x10)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "\n",
    "        # print(np.array(obs[:100]).reshape(10,10))\n",
    "        # print('boxes placed:\\n ',env.boxes)\n",
    "        # obs = env.reset()\n",
    "        # print('done')\n",
    "        break\n",
    "\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m----> 3\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m      4\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[19], line 252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "Cell \u001b[0;32mIn[19], line 279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "Cell \u001b[0;32mIn[19], line 382\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    380\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m    381\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    383\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    385\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    386\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddf6497b5e8b9e5a0a2a825c81e31309ac4b7a979a0f4843c1f4f7cf03ff2cb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
